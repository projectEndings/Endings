
### 5.1 Data

Data is the expression of the source information, knowledge, and expertise of our researchers, and forms the basis and “necessary context for analytical claims” (Thieberger in this issue, and Thieberger 2016). Digital Humanities praxis has often tended to blur the distinction between data and products; according to Barats, Schafer, and Fickers, for example, “data management is not just about the data that are used, but also about those, which are produced (metadata, results of research, etc.). In fact, data management covers the whole life cycle of a research project, from data selection to their curation and description, from analyse and interpreting data to its publication and long term storage” (para 4). For the purpose of Endings principles, our definition of data is much more constrained: we define data as the source material from which end-user products are generated. In the case of a digital edition, data may take the form of TEI XML encodings of primary source or born-digital documents, but it may also include relational database data, digital images, audio or video recordings, and any other material acquired or generated during research, but which does not form part of the end product.

#### 5.1.1 Data is stored only in formats which conform to open standards and which are amenable to processing (TEI XML, GML, ODF, TXT).

It is now common practice for funding agencies and host institutions to insist that projects create detailed data management plans. The Canadian _Tri-Agency Statement of Principles on Digital Data Management_, for example, specifies that “data should be collected and stored throughout the research project using software and formats that ensure secure storage, and enable preservation of and access to the data well beyond the duration of the research project” (Government of Canada 2016). In practice, this means using open, well-documented standards and file formats (i.e. files that can be read, opened, and processed without the need of special software or tools) instead of proprietary formats.

#### 5.1.2 Data is subject to version control (Subversion, Git).

The use of version control for DH data is now widely accepted practice; by its very nature it supports LOCKSS (every person checking out the repository to work on it creates a new copy), provides security against human error and data corruption, and enables sophisticated progress tracking (Holmes et al. 2019). We also believe that project history, as reflected in progressive changes to its dataset, merits preservation in itself, and version control systems support this. [XREF (JT): Reference to JM's paper/presentation on student labour? Version control also allows for granular responsibility for a given file. And maybe a footnote to [*Gource*](https://gource.io/) and MoEML's visualization?] <!--Note: This is also the first reference to LOCKSS, which should be defined but a bit tricky since it is outlined in better detail in 05_4.1-->

#### 5.1.3 Data is continually subject to validation and diagnostic analysis.

Since all products depend entirely on the reliability of the project's dataset, it is essential to establish and maintain the accuracy of the data. For XML datasets, all files should be validated against the appropriate schemata; for other types of data, routines should be built into processing that confirms that the project’s data conforms to the project’s determined syntax and convention.^[We discuss this processing in greater detail in 5.4] In addition to confirming that individual files conform to the project’s schema, the entire dataset should also be checked using a set of of what we have previously called “diagnostics” (Holmes and Takeda 2019b). As we argued, project’s contain multiple internal and external relationships—to project-defined entities, other pages within the project, or external vocabularies—that “need to be tested, checked, and validated, too, but it is impractical to do this using document-level schemas” (i101–2). Taken together, standard document validation and diagnostics are necessary for confirming that a project is consistent, coherent, and complete.
