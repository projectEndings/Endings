<!--Archiving. Now it's built, how do we archive it? Negotiating with the Library. The issue with URLs: is a domain name intrinsic to the identity of the project? Will the archive [pay to] maintain it?-->


## 7. Archiving

The importance of secure archival storage has long been recognized, especially in the library community. Back in 2002, Gelaw et al. noted that “It is now common knowledge that digital information is fragile in ways that differ from traditional technologies, such as paper or microfilm. The fact that information is increasingly stored in digital form,[sic] has led to an accelerated search for effective methods of managing electronic information resources.” However, the very title of that article, “A Metadata Approach to Preservation of Digital Resources,” underscores the fact that for librarians, archiving is primarily a matter of metadata; “creation of preservation metadata for electronic files and digital collections are among the most important steps.” The assumption has been that digital resources are discrete blobs to be stored in large repositories; accessibility and findability depend on the provision of effective descriptive metadata. Libraries have traditionally viewed digital archiving as a storage and replication issue; “in order to ensure longevity, the library will make multiple copies of our digital collections, aggregate them into Archival Information Packages (AIPs), and distribute these archival copies in offsite locations” (Goddard and Seeman 2020). 

For end-users not able to access archiving services from libraries or similar institutions, there are also archiving services available on the Web, such as Zenodo (offered by CERN, [url](https://zenodo.org/)), Preprints.org, or HAL ([url](https://hal.archives-ouvertes.fr/)). All of these, however, follow a similar model: a single binary object such as an article is uploaded and associated with metadata. Datasets may also be attached, but these are viewed as objects to download. This kind of archiving, while it may be a useful backup strategy (and we can never have enough of those), will not make a digital edition project (i.e. a complex website) available in the way a user would want to access it: as a functioning website. The Text Encoding Initiative, for example, archives each version of its standards package, consisting of schemas, exemplars and other documentation, on Zenodo, but the end user can only download a zipped package of materials; they cannot browse and read the TEI Guidelines itself, as they can on the TEI's own website.

In other words, policy development and infrastructure provision for digital archiving initiatives up to this point has largely been focused on data storage and management (see Molloy 2011 or InterPARES 2013 for examples). But when it comes to archiving large digital edition projects, this sort of preservation really serves only one component of our needs: reliable backup. Of course we should not underestimate the importance of backup. But as we have suggested above, if a digital edition is to remain relevant, to be read and cited, and to hold its place in the scholarly dialogue, it must be fully functional, available, and usable; after all, “Humanities scholars have been taught that what they produce should be, first and foremost, consumed and understood by other scholars. Once this is accomplished, they want those who have received their work to reuse or build upon it, so that their contributions become part of a larger conversation, one that may stretch out for years, decades, or even centuries” (Morreale 2019, 4). A shelved DVD copy with metadata in a catalogue will not facilitate this; only a functional website will enable this conversation. Saachi (2015), based on a now-inaccessible technical report from the National Archives of Australia, characterizes direct, live accessibility to digital objects as “performance” (51ff), and contrasts it with “stable artefacts”. Interaction with digital resources in the performance mode is always a unique experience bound to a specific occasion, and is contingent on the availability of hardware and software components which are transient.^[ See Warwick 2020 for a recent historical analysis of digital humanities interface and for a discussion of the challenges presented by an incomplete or unusable archived copy of a project.] “The NAA Performance Model explicitly addresses the need for something concrete that needs to experienced by a researcher in order access the essence of a record: a performance” (53). 

From an Endings perspective, then, although it is essential to be able to preserve the “stable artefact” in the form of an archive such as a zip file with associated metadata, preserving the “essence” of a digital edition really lies in maintaining its functionality so that users can experience it through “performance”, or interaction. 

Librarians and archivists are now beginning to focus on this important distinction. The University of Victoria Library is in the process of developing a “Donation policy for legacy websites,” which attempts to lay out the conditions under which an individual or a project group may transfer an entire website to the library for long-term storage, with the expectation that it will actually remain accessible as a website on the Internet for public access. Offering such a service is not simple, though. A library presented with a Tamagotchi-style web application, with dependencies on back-end databases and server-side scripting, will (and should, quite rightly) reject the “donation,” which would actually amount to the passing on of technical debt and the obligation for incessant maintenance. But the static site model we have proposed and implemented is not subject to these considerations. Assuming any functional webserver is available, a static site can be served for the foreseeable future with no maintenance burden whatsoever; only bandwidth and disk storage are required, just as is the case for binary-blob digital assets in the traditional archive model. With a static-site approach, we can imagine a situation in which any given edition of a digital collection might be available in multiple “locations” (i.e. at multiple URLs), hosted by different institutions, providing resilient and reliable access to the resource even in the face of occasional failures or closures at specific institutions.

Such a scenario presents its own challenges, though. If there are multiple incarnations of a site, how is it to be cited, and how is a reader to know that they are viewing the “same” site as that cited in a publication, albeit perhaps on a different URL? This is one reason that the edition management and release model proposed in section 5.5 is so crucial. Each coherent, consistent and complete edition of a digital project may be archived and served from many locations, so it must be reliably and obviously consistent and show clear dating and versioning information, just like an edition of a printed book. 







